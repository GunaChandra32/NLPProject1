{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JIR2MeSsLrLX"
      },
      "outputs": [],
      "source": [
        "with open('block_chain.txt', 'r') as file:\n",
        "    data = file.read().replace('\\n', '')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total Lines in file: {len(data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMjqJOo6LvvO",
        "outputId": "bbb5d0fd-0c84-440b-dbda-b9994431e158"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Lines in file: 66561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.util import ngrams\n",
        "import random"
      ],
      "metadata": {
        "id": "Nyo6mcxOLyco"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIFu83YiL0lu",
        "outputId": "ca3b3185-b83f-49b9-d8d8-38659cd59c0f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "oti0TtibL3i-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = word_tokenize(data)\n",
        "print(f\"Total Tokens: {len(tokens)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txO8dq7GL7qD",
        "outputId": "071bb673-5d3c-4bc6-ff17-a2f690d72e5e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Tokens: 11271\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [word for word in tokens if word.lower() not in stop_words and word.isalpha()]\n",
        "print(f\"Total Tokens after removing StopWords: {len(tokens)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knb4dqmcL8mX",
        "outputId": "b60aad85-96f0-461a-caa0-9af5edc96dd4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Tokens after removing StopWords: 5554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bigram model using NTLK function\n",
        "bigrams = list(ngrams(tokens, 2))\n",
        "\n",
        "requried_sentences = 5\n",
        "generated_sentence_count=0\n",
        "\n",
        "while requried_sentences:\n",
        "  #Randomly selecting starting_word\n",
        "  starting_word = random.choice(tokens)\n",
        "\n",
        "  # Generating a sentence of 10 words using bigram model\n",
        "  sentence = [starting_word]\n",
        "  required_words_in_sentence = 10\n",
        "  for i in range(required_words_in_sentence):\n",
        "      # Generating all possible bigrams using previous word in the sentence.\n",
        "      possible_bigrams = [bigram for bigram in bigrams if bigram[0] == sentence[-1]]\n",
        "\n",
        "      if len(possible_bigrams) == 0:\n",
        "          # If no possible bigrams were generated, We need to choose random starting_word again and Repeat above steps.\n",
        "          starting_word = random.choice(tokens)\n",
        "          sentence = [starting_word]\n",
        "          continue\n",
        "          \n",
        "      # Randomly choosing a word from generated possible bigrams\n",
        "      chosen_bigram = random.choice(possible_bigrams)\n",
        "      \n",
        "      # Adding te word in tho the sentence.\n",
        "      sentence.append(chosen_bigram[1])\n",
        "      \n",
        "  # Joining words to sentence\n",
        "  generated_sentence = ' '.join(sentence)\n",
        "  generated_sentence_count+=1\n",
        "  # Generated Sentence\n",
        "  print(f\"{generated_sentence_count}: {generated_sentence}\")\n",
        "  requried_sentences-=1\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tq-RmQdgL_XU",
        "outputId": "7eea1781-b6f6-428c-ea8b-6e45d653d272"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: code removing duplicates contracts increased time contributed token selling comparison blockchain\n",
            "2: programs crowdfunding platforms like exchange burn programs used tokens used solving\n",
            "3: reason systems report peer review infrastructure considers deployment different technical side\n",
            "4: main manifestation includes smart contracts performing benchmarks stress testing software component\n",
            "5: smart contracts particular interesting Erthereum blockchain complete awareness potential performance Towards\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# trigram model using NTLK function\n",
        "trigrams = list(ngrams(tokens, 3))\n",
        "\n",
        "requried_sentences = 5\n",
        "generated_sentence_count=0\n",
        "\n",
        "while requried_sentences:\n",
        "  #Randomly selecting starting_word\n",
        "  starting_word = random.choice(tokens)\n",
        "\n",
        "  # Generating a sentence of 10 words using trigram model\n",
        "  sentence = [starting_word]\n",
        "  required_words_in_sentence = 10\n",
        "  for i in range(required_words_in_sentence):\n",
        "      # Generating all possible trigrams using previous word in the sentence.\n",
        "      possible_trigrams = [trigram for trigram in trigrams if trigram[0] == sentence[-1]]\n",
        "\n",
        "      if len(possible_trigrams) == 0:\n",
        "          # If no possible trigrams were generated, We need to choose random starting_word again and Repeat above steps.\n",
        "          starting_word = random.choice(tokens)\n",
        "          sentence = [starting_word]\n",
        "          continue\n",
        "          \n",
        "      # Randomly choosing a word from generated possible trigrams\n",
        "      chosen_trigram = random.choice(possible_trigrams)\n",
        "      \n",
        "      # Adding te word in tho the sentence.\n",
        "      sentence.append(chosen_trigram[1])\n",
        "      \n",
        "  # Joining words to sentence\n",
        "  generated_sentence = ' '.join(sentence)\n",
        "  generated_sentence_count+=1\n",
        "  # Generated Sentence\n",
        "  print(f\"{generated_sentence_count}: {generated_sentence}\")\n",
        "  requried_sentences-=1\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6GwsvkqMB_e",
        "outputId": "ac31ca1d-48cd-439e-a9de-9356ec2a745d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: use digital finance securities industry provides new peer review data regards\n",
            "2: trust protocol deploy date smart contracts feasible safe tunnels c Accounts\n",
            "3: ensure Solidity files bootstrapping private key blockchain protocol ensure validity also\n",
            "4: journals published Many researchers efficiently systematically access rules different verified blockchain\n",
            "5: unless standard libraries like SafeMath Oraclize listed previously smart contracts ConclusionFor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# four_gram model using NTLK function\n",
        "four_grams = list(ngrams(tokens, 4))\n",
        "\n",
        "requried_sentences = 5\n",
        "generated_sentence_count=0\n",
        "\n",
        "while requried_sentences:\n",
        "  #Randomly selecting starting_word\n",
        "  starting_word = random.choice(tokens)\n",
        "\n",
        "  # Generating a sentence of 10 words using four_gram model\n",
        "  sentence = [starting_word]\n",
        "  required_words_in_sentence = 10\n",
        "  for i in range(required_words_in_sentence):\n",
        "      # Generating all possible four_grams using previous word in the sentence.\n",
        "      possible_four_grams = [four_gram for four_gram in four_grams if four_gram[0] == sentence[-1]]\n",
        "\n",
        "      if len(possible_four_grams) == 0:\n",
        "          # If no possible four_grams were generated, We need to choose random starting_word again and Repeat above steps.\n",
        "          starting_word = random.choice(tokens)\n",
        "          sentence = [starting_word]\n",
        "          continue\n",
        "          \n",
        "      # Randomly choosing a word from generated possible four_grams\n",
        "      chosen_four_gram = random.choice(possible_four_grams)\n",
        "      \n",
        "      # Adding te word in tho the sentence.\n",
        "      sentence.append(chosen_four_gram[1])\n",
        "      \n",
        "  # Joining words to sentence\n",
        "  generated_sentence = ' '.join(sentence)\n",
        "  generated_sentence_count+=1\n",
        "  # Generated Sentence\n",
        "  print(f\"{generated_sentence_count}: {generated_sentence}\")\n",
        "  requried_sentences-=1\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Iwc5c4-ME6k",
        "outputId": "75c631fc-6ca0-4859-f81f-aa59eb2c8a93"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: include ICO Crowdsales Certification NFT CNFT appearance increased time emphasized smart\n",
            "2: inconsistency ineffective detecting errors network node computing pressure Mass data security\n",
            "3: samples know priorly category used set gambling games possible change information\n",
            "4: terms samples increases difficulty supervision Blockchain shown effective improving discoverability peer\n",
            "5: equivalent data visualizations looking transactions without database However continuous integration open\n"
          ]
        }
      ]
    }
  ]
}